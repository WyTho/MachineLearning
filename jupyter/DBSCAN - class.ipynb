{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pylab import rcParams\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.cluster import DBSCAN\n",
    "from collections import Counter\n",
    "import datetime\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameValidator:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    \n",
    "    def validate(columns = [], minimumRowAmount=1):\n",
    "        # TODO: validate columns\n",
    "        # TODO: validate rows\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_milliseconds = lambda seconds : seconds * 60 * 1000\n",
    "\n",
    "class BinaryDataAnalysis:\n",
    "    def __init__(self,\n",
    "                 eps=5, #minutes\n",
    "                 cluster_degregation=2,\n",
    "                 max_cluster_distance=7.5,   #minutes\n",
    "                 weeks=5,\n",
    "                 decay_strength=0.5,\n",
    "                 cluster_threshold=25,\n",
    "                 threshold_percentage=90):\n",
    "        self.eps =                  eps\n",
    "        self.cluster_degregation =  cluster_degregation\n",
    "        self.max_cluster_distance = max_cluster_distance\n",
    "        self.weeks =                weeks\n",
    "        self.decay_strength =       decay_strength\n",
    "        self.cluster_threshold =    cluster_threshold\n",
    "        self.threshold_percentage = threshold_percentage\n",
    "        \n",
    "    def analyze(self, df):\n",
    "        \n",
    "        self.lookup_table = self.create_lookup_table(\n",
    "            df=df\n",
    "        )\n",
    "        \n",
    "        df_fit = self.clean_dataframe(\n",
    "            df=df\n",
    "        )\n",
    "        week_hashcodes = self.get_week_clusters_hash_codes(\n",
    "            df=df_fit\n",
    "        )\n",
    "        hashcode_occurances = self.get_hashcode_occurances_per_week(\n",
    "            week_hashcodes=week_hashcodes\n",
    "        )\n",
    "        predicted_groups = self.calculate_groups(\n",
    "            hashcode_occurances_per_week=hashcode_occurances\n",
    "        )\n",
    "        \n",
    "        result = []\n",
    "        for key in predicted_groups:\n",
    "            items = self.get_lookup_values(\n",
    "                hashcode=key\n",
    "            )\n",
    "            result.append({\n",
    "                'item_ids': items,\n",
    "                'is_predicted_group_percentage': predicted_groups[key]['is_predicted_group_percentage'],\n",
    "                'is_relevant_group_percentage': predicted_groups[key]['is_relevant_group_percentage']\n",
    "            })\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def create_lookup_table(self, df):\n",
    "                \n",
    "        df_lookup = pd.DataFrame(data={ 'id': pd.Series(df['id']).unique() })\n",
    "        \n",
    "        df_lookup['hashcode'] = self.clean_dataframe(\n",
    "            df=df_lookup\n",
    "        )['id']\n",
    "        lookup_dict = dict()\n",
    "        for index, row in df_lookup.iterrows():\n",
    "            lookup_dict[row['hashcode']] = row['id']\n",
    "        return lookup_dict\n",
    "    \n",
    "    def clean_dataframe(self, df):\n",
    "        \"\"\"Convert non-nummeric values in the dataframe to numbers so that the dataframe can be used to fit a model\n",
    "\n",
    "        Args:\n",
    "            df: The dataframe to clean.\n",
    "\n",
    "        Returns:\n",
    "            df_fit: The dataframe with nummeric values\n",
    "        \"\"\"\n",
    "        d = defaultdict(LabelEncoder)\n",
    "        df_fit = df.apply(lambda x: d[x.name].fit_transform(x))\n",
    "        if 'state' in df.columns:\n",
    "            df_fit['state'] = df['state']\n",
    "        if 'time' in df.columns:\n",
    "            df_fit['time'] = df['time']\n",
    "        return df_fit\n",
    "    \n",
    "    def get_week_clusters_hash_codes(self, df):\n",
    "        \"\"\"Get Cluster for a dataframe per week\n",
    "\n",
    "        Args:\n",
    "            df: The dataframe with more than one week of timestamps to cluster.\n",
    "\n",
    "        Returns:\n",
    "            TODO: CHANGE: cluster_arr: An array of weeks (arrays) that each hold 0 or more dataframes (clusters)\n",
    "        \"\"\"\n",
    "        one_week_in_milliseconds = (1000 * 60 * 60 * 24 * 7)\n",
    "        last_timestamp = df['time'].max()\n",
    "        week_hashcodes = []\n",
    "        for week in range(self.weeks):\n",
    "            week_hashcodes.append([])\n",
    "            df_week = df[df['time'] >= last_timestamp - ((week + 1) * one_week_in_milliseconds)]\n",
    "            df_week = df_week[df_week['time'] < last_timestamp - (week * one_week_in_milliseconds)]\n",
    "\n",
    "            if not df_week.empty:\n",
    "                cluster_arr = self.split_dataframe_on_state_and_get_cluster_arr(\n",
    "                    df=df_week, \n",
    "                    starting_eps=self.eps\n",
    "                )\n",
    "                for idx, df_week in enumerate(cluster_arr):\n",
    "                    cluster = []\n",
    "                    for row in df_week.iterrows():\n",
    "                        index, data = row\n",
    "                        cluster.append(data['id'].tolist())\n",
    "\n",
    "                    cluster = list(set(cluster))\n",
    "\n",
    "                    hashcode = 0\n",
    "                    for lamp in cluster:\n",
    "                        hashcode += pow(2, lamp)\n",
    "\n",
    "                    if(len(cluster) > 1):\n",
    "                        week_hashcodes[week].append(hashcode)\n",
    "            else:\n",
    "                print('WARNING!!! There are not', self.weeks, 'weeks in the dataset... amount_of_weeks HAS BEEN CHANGED TO', week)\n",
    "                self.weeks = week\n",
    "                break\n",
    "        return week_hashcodes\n",
    "    \n",
    "    def split_dataframe_on_state_and_get_cluster_arr(self, df, starting_eps):\n",
    "        \"\"\"Split a dataframe into 2 seperate dataframes (one with state=0, the other with state=1) \n",
    "           and get the clusters for both of the dataframes\n",
    "\n",
    "        Args:\n",
    "            df: The dataframe to split & get clusters from.\n",
    "\n",
    "        Returns:\n",
    "            cluster_arr: an array that holds 0 or more dataframes (clusters)\n",
    "        \"\"\"\n",
    "        df_1 = df.loc[df['state'] == 1]\n",
    "        df_0 = df.loc[df['state'] == 0]\n",
    "        cluster_arr1 = self.get_clusters_recursive(df=df_1.copy(), eps=self.eps)\n",
    "        cluster_arr2 = self.get_clusters_recursive(df=df_0.copy(), eps=self.eps)\n",
    "        cluster_arr = cluster_arr1 + cluster_arr2\n",
    "        return cluster_arr\n",
    "    \n",
    "    def get_clusters_recursive(self, df, eps, iteration=0, cluster_arr=None):\n",
    "        if cluster_arr is None:\n",
    "            cluster_arr = []\n",
    "        \n",
    "        model = self.fit_model(df, eps)\n",
    "        cluster_dict = self.get_clusters(df=df, model=model)\n",
    "        \n",
    "        for idx, df in cluster_dict['too_large'].items():\n",
    "            cluster_arr + self.get_clusters_recursive(\n",
    "                df=cluster_dict['too_large'][idx], \n",
    "                eps=eps / self.cluster_degregation, \n",
    "                iteration=iteration + 1, \n",
    "                cluster_arr=cluster_arr\n",
    "            )\n",
    "    \n",
    "        for idx, df in cluster_dict['perfect_size'].items():\n",
    "            cluster_arr.append(df)\n",
    "        return cluster_arr\n",
    "    \n",
    "    \n",
    "    def fit_model(self, df, eps):\n",
    "        model = DBSCAN(\n",
    "            eps=to_milliseconds(eps),\n",
    "            min_samples=2\n",
    "        ).fit(df)\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def get_clusters(self, df, model):\n",
    "        \n",
    "        df['cluster'] = model.labels_\n",
    "        \n",
    "        cluster_dict_too_large = {}\n",
    "        cluster_dict_perfect_size = {}\n",
    "        \n",
    "        \n",
    "        # Calculate amount of clusters\n",
    "        cluster_data_count = Counter(model.labels_)\n",
    "        if -1 in cluster_data_count:\n",
    "            cluster_data_count.pop(-1) # don't count outliers as a cluster\n",
    "        if (bool(cluster_data_count)):\n",
    "            amount_of_clusters = max(cluster_data_count) + 1\n",
    "        else:\n",
    "            amount_of_clusters = 0;\n",
    "        \n",
    "        \n",
    "        for idx in range(amount_of_clusters):\n",
    "            cluster_df = df.loc[df['cluster'] == idx].drop(columns=['cluster'])\n",
    "            \n",
    "            first_time = cluster_df['time'].iloc[0]\n",
    "            last_time = cluster_df['time'].iloc[cluster_df['time'].size - 1]\n",
    "            diffrence_in_miliseconds = last_time - first_time\n",
    "            if diffrence_in_miliseconds > to_milliseconds(self.max_cluster_distance):\n",
    "                cluster_dict_too_large[idx] = cluster_df\n",
    "            else:\n",
    "                cluster_dict_perfect_size[idx] = cluster_df\n",
    "        \n",
    "        return {\n",
    "            'too_large': cluster_dict_too_large,\n",
    "            'perfect_size': cluster_dict_perfect_size\n",
    "        }\n",
    "        \n",
    "    \n",
    "    def get_hashcode_occurances_per_week(self, week_hashcodes):\n",
    "        count_dict = {}\n",
    "        for week, hashcodes_arr in enumerate(week_hashcodes):\n",
    "            for i in hashcodes_arr:\n",
    "                if i in count_dict:\n",
    "                    count_dict[i]['occurance_week'][str(week)] += 1\n",
    "                else:\n",
    "                    count_dict[i] = {}\n",
    "                    count_dict[i]['occurance_week'] = {}\n",
    "                    for w in range(self.weeks):\n",
    "                        count_dict[i]['occurance_week'][str(w)] = 0\n",
    "        return count_dict\n",
    "    \n",
    "    def calculate_groups(self, hashcode_occurances_per_week):\n",
    "        \n",
    "        # TODO: clean this up\n",
    "        \n",
    "        count_dict = hashcode_occurances_per_week\n",
    "        for key,val in count_dict.items():\n",
    "            threshold = self.cluster_threshold * self.weeks\n",
    "\n",
    "            total_occurances = 0\n",
    "            for week in range(self.weeks):\n",
    "                total_occurances += val['occurance_week'][str(week)]\n",
    "\n",
    "            if total_occurances >= threshold:\n",
    "                div = (total_occurances / threshold)\n",
    "                count = 1\n",
    "                perc = self.threshold_percentage\n",
    "\n",
    "                while div > 1:\n",
    "                    div /= 2\n",
    "                    perc += ((100 - self.threshold_percentage) / 2) * (1 / count)\n",
    "                    count *= 2\n",
    "\n",
    "            else:\n",
    "                perc = (total_occurances / threshold) * self.threshold_percentage\n",
    "\n",
    "            count_dict[key]['is_predicted_group_percentage'] = round(perc, 2)\n",
    "\n",
    "\n",
    "        for key,val in count_dict.items():\n",
    "            total = 0\n",
    "            current = 0\n",
    "            for week in range(self.weeks):\n",
    "\n",
    "                perc = 0\n",
    "                if val['occurance_week'][str(week)] >= self.cluster_threshold:\n",
    "                    div = (val['occurance_week'][str(week)] / self.cluster_threshold)\n",
    "                    count = 1\n",
    "                    perc = self.threshold_percentage\n",
    "                    while div > 1:\n",
    "                        div /= 2\n",
    "                        perc += ((100 - self.threshold_percentage) / 2) * (1 / count)\n",
    "                        count *= 2\n",
    "                else:\n",
    "                    perc = (val['occurance_week'][str(week)] / self.cluster_threshold) * self.threshold_percentage\n",
    "\n",
    "                total += 100 * (0.5) / pow(2, week * self.decay_strength)\n",
    "                current += perc * (0.5) / pow(2, week * self.decay_strength)\n",
    "\n",
    "            count_dict[key]['is_relevant_group_percentage'] = round((current / total) * 100, 2)\n",
    "            count_dict[key].pop('occurance_week', None)\n",
    "        return count_dict\n",
    "    \n",
    "    def get_lookup_values(self, hashcode):\n",
    "        def bitfield(n):\n",
    "            return [int(digit) for digit in bin(n)[2:]]\n",
    "            \n",
    "        bits = bitfield(hashcode)[::-1]\n",
    "        \n",
    "        items = []\n",
    "        for idx, bit in enumerate(bits):\n",
    "            if bit == 1:\n",
    "                items.append(self.lookup_table[idx])\n",
    "        return items\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>time</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1509489940655</td>\n",
       "      <td>Staande_Lamp_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1509490011225</td>\n",
       "      <td>Staande_Lamp_5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1509491943009</td>\n",
       "      <td>Staande_Lamp_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1509492221471</td>\n",
       "      <td>Staande_Lamp_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1509492826941</td>\n",
       "      <td>Staande_Lamp_3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   state           time              id\n",
       "2      0  1509489940655  Staande_Lamp_3\n",
       "6      1  1509490011225  Staande_Lamp_5\n",
       "0      1  1509491943009  Staande_Lamp_1\n",
       "1      0  1509492221471  Staande_Lamp_2\n",
       "3      1  1509492826941  Staande_Lamp_3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "address = '../datasets/staandelamp_realistic.json'\n",
    "df_data = pd.read_json(address)\n",
    "df_data = df_data.sort_values(by=['time'])\n",
    "df_data['id'] = df_data['name']\n",
    "df_data = df_data.drop(columns=['name'])\n",
    "print(df_data.shape)\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'item_ids': ['Staande_Lamp_3', 'Staande_Lamp_5'],\n",
       "  'is_predicted_group_percentage': 95.0,\n",
       "  'is_relevant_group_percentage': 92.63},\n",
       " {'item_ids': ['Staande_Lamp_1', 'Staande_Lamp_5'],\n",
       "  'is_predicted_group_percentage': 32.4,\n",
       "  'is_relevant_group_percentage': 31.89},\n",
       " {'item_ids': ['Staande_Lamp_1', 'Staande_Lamp_3'],\n",
       "  'is_predicted_group_percentage': 95.0,\n",
       "  'is_relevant_group_percentage': 92.38},\n",
       " {'item_ids': ['Staande_Lamp_1', 'Staande_Lamp_4', 'Staande_Lamp_5'],\n",
       "  'is_predicted_group_percentage': 2.88,\n",
       "  'is_relevant_group_percentage': 3.47},\n",
       " {'item_ids': ['Staande_Lamp_2', 'Staande_Lamp_3'],\n",
       "  'is_predicted_group_percentage': 87.84,\n",
       "  'is_relevant_group_percentage': 87.26},\n",
       " {'item_ids': ['Staande_Lamp_2', 'Staande_Lamp_5'],\n",
       "  'is_predicted_group_percentage': 21.6,\n",
       "  'is_relevant_group_percentage': 20.51},\n",
       " {'item_ids': ['Staande_Lamp_3', 'Staande_Lamp_4'],\n",
       "  'is_predicted_group_percentage': 90,\n",
       "  'is_relevant_group_percentage': 80.89},\n",
       " {'item_ids': ['Staande_Lamp_2', 'Staande_Lamp_4'],\n",
       "  'is_predicted_group_percentage': 29.52,\n",
       "  'is_relevant_group_percentage': 31.49},\n",
       " {'item_ids': ['Staande_Lamp_4', 'Staande_Lamp_5'],\n",
       "  'is_predicted_group_percentage': 23.04,\n",
       "  'is_relevant_group_percentage': 24.05},\n",
       " {'item_ids': ['Staande_Lamp_2', 'Staande_Lamp_3', 'Staande_Lamp_4'],\n",
       "  'is_predicted_group_percentage': 12.24,\n",
       "  'is_relevant_group_percentage': 9.99},\n",
       " {'item_ids': ['Staande_Lamp_2', 'Staande_Lamp_3', 'Staande_Lamp_5'],\n",
       "  'is_predicted_group_percentage': 16.56,\n",
       "  'is_relevant_group_percentage': 14.51},\n",
       " {'item_ids': ['Staande_Lamp_1', 'Staande_Lamp_4'],\n",
       "  'is_predicted_group_percentage': 33.12,\n",
       "  'is_relevant_group_percentage': 34.13},\n",
       " {'item_ids': ['Staande_Lamp_1', 'Staande_Lamp_3', 'Staande_Lamp_5'],\n",
       "  'is_predicted_group_percentage': 11.52,\n",
       "  'is_relevant_group_percentage': 12.69},\n",
       " {'item_ids': ['Staande_Lamp_1', 'Staande_Lamp_3', 'Staande_Lamp_4'],\n",
       "  'is_predicted_group_percentage': 12.24,\n",
       "  'is_relevant_group_percentage': 11.44},\n",
       " {'item_ids': ['Staande_Lamp_1', 'Staande_Lamp_2'],\n",
       "  'is_predicted_group_percentage': 23.76,\n",
       "  'is_relevant_group_percentage': 23.25},\n",
       " {'item_ids': ['Staande_Lamp_1', 'Staande_Lamp_2', 'Staande_Lamp_3'],\n",
       "  'is_predicted_group_percentage': 9.36,\n",
       "  'is_relevant_group_percentage': 7.33},\n",
       " {'item_ids': ['Staande_Lamp_2', 'Staande_Lamp_4', 'Staande_Lamp_5'],\n",
       "  'is_predicted_group_percentage': 3.6,\n",
       "  'is_relevant_group_percentage': 3.28},\n",
       " {'item_ids': ['Staande_Lamp_3', 'Staande_Lamp_4', 'Staande_Lamp_5'],\n",
       "  'is_predicted_group_percentage': 9.36,\n",
       "  'is_relevant_group_percentage': 8.88},\n",
       " {'item_ids': ['Staande_Lamp_1', 'Staande_Lamp_2', 'Staande_Lamp_5'],\n",
       "  'is_predicted_group_percentage': 2.16,\n",
       "  'is_relevant_group_percentage': 1.87},\n",
       " {'item_ids': ['Staande_Lamp_1',\n",
       "   'Staande_Lamp_2',\n",
       "   'Staande_Lamp_3',\n",
       "   'Staande_Lamp_4'],\n",
       "  'is_predicted_group_percentage': 2.16,\n",
       "  'is_relevant_group_percentage': 1.09},\n",
       " {'item_ids': ['Staande_Lamp_1', 'Staande_Lamp_2', 'Staande_Lamp_4'],\n",
       "  'is_predicted_group_percentage': 2.16,\n",
       "  'is_relevant_group_percentage': 1.09},\n",
       " {'item_ids': ['Staande_Lamp_1',\n",
       "   'Staande_Lamp_3',\n",
       "   'Staande_Lamp_4',\n",
       "   'Staande_Lamp_5'],\n",
       "  'is_predicted_group_percentage': 0.72,\n",
       "  'is_relevant_group_percentage': 0.32},\n",
       " {'item_ids': ['Staande_Lamp_1',\n",
       "   'Staande_Lamp_2',\n",
       "   'Staande_Lamp_4',\n",
       "   'Staande_Lamp_5'],\n",
       "  'is_predicted_group_percentage': 0.0,\n",
       "  'is_relevant_group_percentage': 0.0}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BDASCAN = BinaryDataAnalysis()\n",
    "result = BDASCAN.analyze(df_data)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
