{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pylab import rcParams\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.cluster import DBSCAN\n",
    "from collections import Counter\n",
    "import datetime\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameValidator:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    \n",
    "    def validate(columns = [], minimumRowAmount=1):\n",
    "        # TODO: validate columns\n",
    "        # TODO: validate rows\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_milliseconds = lambda seconds : seconds * 60 * 1000\n",
    "\n",
    "class BinaryDataAnalysis:\n",
    "    min_samples_untill_its_a_cluster = 2\n",
    "    \n",
    "    def __init__(self,\n",
    "                 eps=5, #minutes\n",
    "                 cluster_degregation=2,\n",
    "                 max_cluster_distance=7.5,   #minutes\n",
    "                 weeks=5,\n",
    "                 decay_strength=0.5,\n",
    "                 cluster_threshold=25,\n",
    "                 threshold_percentage=90):\n",
    "        self.eps =                  eps\n",
    "        self.cluster_degregation =  cluster_degregation\n",
    "        self.max_cluster_distance = max_cluster_distance\n",
    "        self.weeks =                weeks\n",
    "        self.decay_strength =       decay_strength\n",
    "        self.cluster_threshold =    cluster_threshold\n",
    "        self.threshold_percentage = threshold_percentage\n",
    "\n",
    "    def analyze(self, df):\n",
    "        df_fit = self.clean_dataframe(\n",
    "            df=df\n",
    "        )\n",
    "        week_hashcodes = self.get_week_clusters_hash_codes(\n",
    "            df=df_fit\n",
    "        )\n",
    "        hashcode_occurances = self.get_hashcode_occurances_per_week(\n",
    "            week_hashcodes=week_hashcodes\n",
    "        )\n",
    "        result = self.calculate_groups(\n",
    "            hashcode_occurances_per_week=hashcode_occurances\n",
    "        )\n",
    "        return result\n",
    "    \n",
    "    def clean_dataframe(self, df):\n",
    "        \"\"\"Convert non-nummeric values in the dataframe to numbers so that the dataframe can be used to fit a model\n",
    "\n",
    "        Args:\n",
    "            df: The dataframe to clean.\n",
    "\n",
    "        Returns:\n",
    "            df_fit: The dataframe with nummeric values\n",
    "        \"\"\"\n",
    "        d = defaultdict(LabelEncoder)\n",
    "        df_fit = df.apply(lambda x: d[x.name].fit_transform(x))\n",
    "        df_fit['state'] = df['state']\n",
    "        df_fit['time'] = df['time']\n",
    "        return df_fit\n",
    "    \n",
    "    def get_week_clusters_hash_codes(self, df):\n",
    "        \"\"\"Get Cluster for a dataframe per week\n",
    "\n",
    "        Args:\n",
    "            df: The dataframe with more than one week of timestamps to cluster.\n",
    "\n",
    "        Returns:\n",
    "            TODO: CHANGE: cluster_arr: An array of weeks (arrays) that each hold 0 or more dataframes (clusters)\n",
    "        \"\"\"\n",
    "        one_week_in_milliseconds = (1000 * 60 * 60 * 24 * 7)\n",
    "        last_timestamp = df['time'].max()\n",
    "        week_hashcodes = []\n",
    "        for week in range(self.weeks):\n",
    "            week_hashcodes.append([])\n",
    "            df_week = df[df['time'] >= last_timestamp - ((week + 1) * one_week_in_milliseconds)]\n",
    "            df_week = df_week[df_week['time'] < last_timestamp - (week * one_week_in_milliseconds)]\n",
    "\n",
    "            if not df_week.empty:\n",
    "                cluster_arr = self.split_dataframe_on_state_and_get_cluster_arr(\n",
    "                    df=df_week, \n",
    "                    starting_eps=self.eps\n",
    "                )\n",
    "                for idx, df_week in enumerate(cluster_arr):\n",
    "                    cluster = []\n",
    "                    for row in df_week.iterrows():\n",
    "                        index, data = row\n",
    "                        cluster.append(data['name'].tolist())\n",
    "\n",
    "                    cluster = list(set(cluster))\n",
    "\n",
    "                    hashcode = 0\n",
    "                    for lamp in cluster:\n",
    "                        hashcode += pow(2, lamp)\n",
    "\n",
    "                    if(len(cluster) > 1):\n",
    "                        week_hashcodes[week].append(hashcode)\n",
    "            else:\n",
    "                print('WARNING!!! There are not', self.weeks, 'weeks in the dataset... amount_of_weeks HAS BEEN CHANGED TO', week)\n",
    "                self.weeks = week\n",
    "                break\n",
    "        return week_hashcodes\n",
    "    \n",
    "    def split_dataframe_on_state_and_get_cluster_arr(self, df, starting_eps):\n",
    "        \"\"\"Split a dataframe into 2 seperate dataframes (one with state=0, the other with state=1) \n",
    "           and get the clusters for both of the dataframes\n",
    "\n",
    "        Args:\n",
    "            df: The dataframe to split & get clusters from.\n",
    "\n",
    "        Returns:\n",
    "            cluster_arr: an array that holds 0 or more dataframes (clusters)\n",
    "        \"\"\"\n",
    "        df_1 = df.loc[df['state'] == 1]\n",
    "        df_0 = df.loc[df['state'] == 0]\n",
    "        cluster_arr1 = self.get_clusters_recursive(df=df_1.copy(), eps=self.eps)\n",
    "        cluster_arr2 = self.get_clusters_recursive(df=df_0.copy(), eps=self.eps)\n",
    "        cluster_arr = cluster_arr1 + cluster_arr2\n",
    "        return cluster_arr\n",
    "    \n",
    "    def get_clusters_recursive(self, df, eps, iteration=0, cluster_arr=None):\n",
    "        if cluster_arr is None:\n",
    "            cluster_arr = []\n",
    "        \n",
    "        model = self.fit_model(df, eps)\n",
    "        cluster_dict = self.get_clusters(df=df, model=model)\n",
    "        \n",
    "        for idx, df in cluster_dict['too_large'].items():\n",
    "            cluster_arr + self.get_clusters_recursive(\n",
    "                df=cluster_dict['too_large'][idx], \n",
    "                eps=eps / self.cluster_degregation, \n",
    "                iteration=iteration + 1, \n",
    "                cluster_arr=cluster_arr\n",
    "            )\n",
    "    \n",
    "        for idx, df in cluster_dict['perfect_size'].items():\n",
    "            cluster_arr.append(df)\n",
    "        return cluster_arr\n",
    "    \n",
    "    \n",
    "    def fit_model(self, df, eps):\n",
    "        model = DBSCAN(\n",
    "            eps=to_milliseconds(eps),\n",
    "            min_samples=self.min_samples_untill_its_a_cluster\n",
    "        ).fit(df)\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def get_clusters(self, df, model):\n",
    "        \n",
    "        df['cluster'] = model.labels_\n",
    "        \n",
    "        cluster_dict_too_large = {}\n",
    "        cluster_dict_perfect_size = {}\n",
    "        \n",
    "        \n",
    "        # Calculate amount of clusters\n",
    "        cluster_data_count = Counter(model.labels_)\n",
    "        if -1 in cluster_data_count:\n",
    "            cluster_data_count.pop(-1) # don't count outliers as a cluster\n",
    "        if (bool(cluster_data_count)):\n",
    "            amount_of_clusters = max(cluster_data_count) + 1\n",
    "        else:\n",
    "            amount_of_clusters = 0;\n",
    "        \n",
    "        \n",
    "        for idx in range(amount_of_clusters):\n",
    "            cluster_df = df.loc[df['cluster'] == idx].drop(columns=['cluster'])\n",
    "            \n",
    "            first_time = cluster_df['time'].iloc[0]\n",
    "            last_time = cluster_df['time'].iloc[cluster_df['time'].size - 1]\n",
    "            diffrence_in_miliseconds = last_time - first_time\n",
    "            if diffrence_in_miliseconds > to_milliseconds(self.max_cluster_distance):\n",
    "                cluster_dict_too_large[idx] = cluster_df\n",
    "            else:\n",
    "                cluster_dict_perfect_size[idx] = cluster_df\n",
    "        \n",
    "        return {\n",
    "            'too_large': cluster_dict_too_large,\n",
    "            'perfect_size': cluster_dict_perfect_size\n",
    "        }\n",
    "        \n",
    "    \n",
    "    def get_hashcode_occurances_per_week(self, week_hashcodes):\n",
    "        count_dict = {}\n",
    "        for week, hashcodes_arr in enumerate(week_hashcodes):\n",
    "            for i in hashcodes_arr:\n",
    "                if i in count_dict:\n",
    "                    count_dict[i]['occurance_week'][str(week)] += 1\n",
    "                else:\n",
    "                    count_dict[i] = {}\n",
    "                    count_dict[i]['occurance_week'] = {}\n",
    "                    for w in range(self.weeks):\n",
    "                        count_dict[i]['occurance_week'][str(w)] = 0\n",
    "        return count_dict\n",
    "    \n",
    "    def calculate_groups(self, hashcode_occurances_per_week):\n",
    "        \n",
    "        # TODO: clean this up\n",
    "        \n",
    "        count_dict = hashcode_occurances_per_week\n",
    "        for key,val in count_dict.items():\n",
    "            threshold = self.cluster_threshold * self.weeks\n",
    "\n",
    "            total_occurances = 0\n",
    "            for week in range(self.weeks):\n",
    "                total_occurances += val['occurance_week'][str(week)]\n",
    "\n",
    "            if total_occurances >= threshold:\n",
    "                div = (total_occurances / threshold)\n",
    "                count = 1\n",
    "                perc = self.threshold_percentage\n",
    "\n",
    "                while div > 1:\n",
    "                    div /= 2\n",
    "                    perc += ((100 - self.threshold_percentage) / 2) * (1 / count)\n",
    "                    count *= 2\n",
    "\n",
    "            else:\n",
    "                perc = (total_occurances / threshold) * self.threshold_percentage\n",
    "\n",
    "            count_dict[key]['is_predicted_group_percentage'] = round(perc, 2)\n",
    "\n",
    "\n",
    "        for key,val in count_dict.items():\n",
    "            total = 0\n",
    "            current = 0\n",
    "            for week in range(self.weeks):\n",
    "\n",
    "                perc = 0\n",
    "                if val['occurance_week'][str(week)] >= self.cluster_threshold:\n",
    "                    div = (val['occurance_week'][str(week)] / self.cluster_threshold)\n",
    "                    count = 1\n",
    "                    perc = self.threshold_percentage\n",
    "                    while div > 1:\n",
    "                        div /= 2\n",
    "                        perc += ((100 - self.threshold_percentage) / 2) * (1 / count)\n",
    "                        count *= 2\n",
    "                else:\n",
    "                    perc = (val['occurance_week'][str(week)] / self.cluster_threshold) * self.threshold_percentage\n",
    "\n",
    "                total += 100 * (0.5) / pow(2, week * self.decay_strength)\n",
    "                current += perc * (0.5) / pow(2, week * self.decay_strength)\n",
    "\n",
    "            count_dict[key]['is_relevant_group_percentage'] = round((current / total) * 100, 2)\n",
    "            count_dict[key].pop('occurance_week', None)\n",
    "        return count_dict\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>state</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Staande_Lamp_3</td>\n",
       "      <td>0</td>\n",
       "      <td>1509489940655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Staande_Lamp_5</td>\n",
       "      <td>1</td>\n",
       "      <td>1509490011225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Staande_Lamp_1</td>\n",
       "      <td>1</td>\n",
       "      <td>1509491943009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Staande_Lamp_2</td>\n",
       "      <td>0</td>\n",
       "      <td>1509492221471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Staande_Lamp_3</td>\n",
       "      <td>1</td>\n",
       "      <td>1509492826941</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name  state           time\n",
       "2  Staande_Lamp_3      0  1509489940655\n",
       "6  Staande_Lamp_5      1  1509490011225\n",
       "0  Staande_Lamp_1      1  1509491943009\n",
       "1  Staande_Lamp_2      0  1509492221471\n",
       "3  Staande_Lamp_3      1  1509492826941"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "address = '../datasets/staandelamp_realistic.json'\n",
    "df_data = pd.read_json(address)\n",
    "df_data = df_data.sort_values(by=['time'])\n",
    "print(df_data.shape)\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING!!! There are not 9999 weeks in the dataset... amount_of_weeks HAS BEEN CHANGED TO 12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{20: {'is_predicted_group_percentage': 95.0,\n",
       "  'is_relevant_group_percentage': 92.47},\n",
       " 17: {'is_predicted_group_percentage': 30.9,\n",
       "  'is_relevant_group_percentage': 31.9},\n",
       " 5: {'is_predicted_group_percentage': 95.0,\n",
       "  'is_relevant_group_percentage': 91.97},\n",
       " 25: {'is_predicted_group_percentage': 2.4,\n",
       "  'is_relevant_group_percentage': 3.2},\n",
       " 6: {'is_predicted_group_percentage': 95.0,\n",
       "  'is_relevant_group_percentage': 87.7},\n",
       " 18: {'is_predicted_group_percentage': 23.1,\n",
       "  'is_relevant_group_percentage': 21.21},\n",
       " 12: {'is_predicted_group_percentage': 95.0,\n",
       "  'is_relevant_group_percentage': 81.68},\n",
       " 10: {'is_predicted_group_percentage': 28.8,\n",
       "  'is_relevant_group_percentage': 30.69},\n",
       " 24: {'is_predicted_group_percentage': 27.9,\n",
       "  'is_relevant_group_percentage': 26.08},\n",
       " 14: {'is_predicted_group_percentage': 12.9,\n",
       "  'is_relevant_group_percentage': 10.75},\n",
       " 22: {'is_predicted_group_percentage': 12.0,\n",
       "  'is_relevant_group_percentage': 13.63},\n",
       " 9: {'is_predicted_group_percentage': 32.1,\n",
       "  'is_relevant_group_percentage': 34.05},\n",
       " 21: {'is_predicted_group_percentage': 9.6,\n",
       "  'is_relevant_group_percentage': 12.03},\n",
       " 13: {'is_predicted_group_percentage': 11.1,\n",
       "  'is_relevant_group_percentage': 10.98},\n",
       " 3: {'is_predicted_group_percentage': 29.4,\n",
       "  'is_relevant_group_percentage': 25.69},\n",
       " 7: {'is_predicted_group_percentage': 9.6,\n",
       "  'is_relevant_group_percentage': 8.03},\n",
       " 26: {'is_predicted_group_percentage': 4.2,\n",
       "  'is_relevant_group_percentage': 3.28},\n",
       " 28: {'is_predicted_group_percentage': 9.6,\n",
       "  'is_relevant_group_percentage': 8.78},\n",
       " 19: {'is_predicted_group_percentage': 3.0,\n",
       "  'is_relevant_group_percentage': 1.86},\n",
       " 15: {'is_predicted_group_percentage': 1.5,\n",
       "  'is_relevant_group_percentage': 1.08},\n",
       " 11: {'is_predicted_group_percentage': 4.2,\n",
       "  'is_relevant_group_percentage': 1.95},\n",
       " 29: {'is_predicted_group_percentage': 1.2,\n",
       "  'is_relevant_group_percentage': 0.38},\n",
       " 27: {'is_predicted_group_percentage': 0.3,\n",
       "  'is_relevant_group_percentage': 0.03},\n",
       " 23: {'is_predicted_group_percentage': 0.9,\n",
       "  'is_relevant_group_percentage': 0.16},\n",
       " 30: {'is_predicted_group_percentage': 0.6,\n",
       "  'is_relevant_group_percentage': 0.08}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BDASCAN = BinaryDataAnalysis(\n",
    "    weeks=9999\n",
    ")\n",
    "result = BDASCAN.analyze(df_data)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
