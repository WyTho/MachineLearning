{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pylab import rcParams\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.cluster import DBSCAN\n",
    "from collections import Counter\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup data visualisaton params for Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "rcParams['figure.figsize'] = 15, 0.1\n",
    "sb.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "address = '../datasets/staandelamp_realistic.json'\n",
    "df_data = pd.read_json(address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sort the data on timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = df_data.sort_values(by=['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>state</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Staande_Lamp_3</td>\n",
       "      <td>0</td>\n",
       "      <td>1509489940655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Staande_Lamp_5</td>\n",
       "      <td>1</td>\n",
       "      <td>1509490011225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Staande_Lamp_1</td>\n",
       "      <td>1</td>\n",
       "      <td>1509491943009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Staande_Lamp_2</td>\n",
       "      <td>0</td>\n",
       "      <td>1509492221471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Staande_Lamp_3</td>\n",
       "      <td>1</td>\n",
       "      <td>1509492826941</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name  state           time\n",
       "2  Staande_Lamp_3      0  1509489940655\n",
       "6  Staande_Lamp_5      1  1509490011225\n",
       "0  Staande_Lamp_1      1  1509491943009\n",
       "1  Staande_Lamp_2      0  1509492221471\n",
       "3  Staande_Lamp_3      1  1509492826941"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "\n",
    "def clean_dataframe_for_fitting(df):\n",
    "    d = defaultdict(LabelEncoder)\n",
    "    df_fit = df.apply(lambda x: d[x.name].fit_transform(x))\n",
    "    df_fit['state'] = df['state']\n",
    "    df_fit['time'] = df['time']\n",
    "    return df_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the DBSCAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples_untill_its_a_cluster = 2\n",
    "\n",
    "def fit_model(df, eps_distance_in_milliseconds):\n",
    "    model = DBSCAN(\n",
    "        eps=eps_distance_in_milliseconds, \n",
    "        min_samples=min_samples_untill_its_a_cluster\n",
    "    ).fit(df)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get information from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_info(model):\n",
    "    info_dict = {}\n",
    "    info_dict['amount_of_datapoints'] = model.labels_.size\n",
    "    info_dict['amount_of_outliers'] = Counter(model.labels_)[-1]\n",
    "    \n",
    "    \n",
    "    cluster_data_count = Counter(model.labels_)\n",
    "    if -1 in cluster_data_count:\n",
    "        cluster_data_count.pop(-1) # don't count outliers as a cluster\n",
    "    if (bool(cluster_data_count)):\n",
    "        amount_of_clusters = max(cluster_data_count) + 1\n",
    "    else:\n",
    "        amount_of_clusters = 0;\n",
    "    info_dict['amount_of_clusters'] = amount_of_clusters\n",
    "    info_dict['datapoints_per_cluster_dict'] = Counter(model.labels_)\n",
    "    return info_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get multiple datasets (one per cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe_on_cluster(model, df):\n",
    "    df['cluster'] = model.labels_\n",
    "    \n",
    "    cluster_dict = {}\n",
    "    \n",
    "    amount_of_clusters = get_model_info(model)['amount_of_clusters']\n",
    "    \n",
    "    for idx in range(amount_of_clusters):\n",
    "        cluster_dict[idx] = df.loc[df['cluster'] == idx].drop(columns=['cluster'])\n",
    "\n",
    "    return cluster_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all clusters (as dataframes) that are too large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_too_large_clusters(cluster_dict, limit_in_milliseconds):\n",
    "    too_large_clusters_dataframes_dict = {}\n",
    "\n",
    "    for idx, df in cluster_dict.items():\n",
    "\n",
    "        first_time = df['time'].iloc[0]\n",
    "        last_time = df['time'].iloc[df['time'].size - 1]\n",
    "\n",
    "        diffrence_in_miliseconds = last_time - first_time\n",
    "        # diffrence_in_minutes = diffrence_in_miliseconds / 1000 / 60\n",
    "\n",
    "        if diffrence_in_miliseconds > limit_in_milliseconds:\n",
    "            too_large_clusters_dataframes_dict[idx] = df\n",
    "\n",
    "    return too_large_clusters_dataframes_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all clusters (as dataframes) that are not too large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perfect_size_clusters(cluster_dict, limit_in_milliseconds):\n",
    "    perfect_size_clusters_dataframes_dict = {}\n",
    "\n",
    "    for idx, df in cluster_dict.items():\n",
    "\n",
    "        first_time = df['time'].iloc[0]\n",
    "        last_time = df['time'].iloc[df['time'].size - 1]\n",
    "\n",
    "        diffrence_in_miliseconds = last_time - first_time\n",
    "        # diffrence_in_minutes = diffrence_in_miliseconds / 1000 / 60\n",
    "\n",
    "        if diffrence_in_miliseconds <= limit_in_milliseconds:\n",
    "            perfect_size_clusters_dataframes_dict[idx] = df\n",
    "\n",
    "    return perfect_size_clusters_dataframes_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables for the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimum distance between datapoints\n",
    "starting_eps = 5 * 60 * 1000\n",
    "    # 5 minutes\n",
    "\n",
    "    \n",
    "next_eps_division = 2\n",
    "    # 5 minutes -> 2.5 minutes -> 1.25 minutes\n",
    "\n",
    "    \n",
    "max_cluster_distance = 7.5 * 60 * 1000\n",
    "\n",
    "# amount of weeks to analyse\n",
    "amount_of_weeks = 50\n",
    "\n",
    "# lol\n",
    "week_threshold = 25\n",
    "threshold_perc = 90   \n",
    "relevance_decay_strength = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (300000 milliseconds = 5 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING!!! amount_of_weeks HAS BEEN CHANGED TO 12\n",
      "week 0 220\n",
      "week 1 206\n",
      "week 2 208\n",
      "week 3 208\n",
      "week 4 198\n",
      "week 5 216\n",
      "week 6 220\n",
      "week 7 209\n",
      "week 8 183\n",
      "week 9 225\n",
      "week 10 210\n",
      "week 11 168\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "def do_shit(df, eps, iteration=0, cluster_arr=None):\n",
    "    if cluster_arr is None:\n",
    "        cluster_arr = []\n",
    "    \n",
    "    model = fit_model(df, eps)\n",
    "    cluster_dict = split_dataframe_on_cluster(model, df)\n",
    "    too_large_clusters_dict = get_too_large_clusters(cluster_dict, max_cluster_distance)\n",
    "    perfect_size_clusters = get_perfect_size_clusters(cluster_dict, max_cluster_distance)\n",
    "    \n",
    "    for idx, df in too_large_clusters_dict.items():\n",
    "        cluster_arr + do_shit(too_large_clusters_dict[idx], eps / next_eps_division, iteration + 1, cluster_arr)\n",
    "    \n",
    "    for idx, df in perfect_size_clusters.items():\n",
    "        cluster_arr.append(df)\n",
    "    return cluster_arr\n",
    "\n",
    "one_week_in_milliseconds = (1000 * 60 * 60 * 24 * 7)\n",
    "last_timestamp = df_data['time'].max()\n",
    "\n",
    "weeks_clusters = []\n",
    "for week in range(amount_of_weeks):\n",
    "    \n",
    "    df_week_x = df_data[df_data['time'] >= last_timestamp - ((week + 1) * one_week_in_milliseconds)]\n",
    "    df_week_x = df_week_x[df_week_x['time'] < last_timestamp - (week * one_week_in_milliseconds)]\n",
    "    if not df_week_x.empty:\n",
    "        df_fit_1 = clean_dataframe_for_fitting(df_week_x.loc[df_week_x['state'] == 1])\n",
    "        df_fit_0 = clean_dataframe_for_fitting(df_week_x.loc[df_week_x['state'] == 0])\n",
    "\n",
    "        cluster_arr1 = do_shit(df_fit_1, starting_eps)\n",
    "        cluster_arr2 = do_shit(df_fit_0, starting_eps)\n",
    "\n",
    "        cluster_arr = cluster_arr1 + cluster_arr2\n",
    "\n",
    "        weeks_clusters.append(cluster_arr)\n",
    "    else:\n",
    "        amount_of_weeks = week\n",
    "        break\n",
    "        \n",
    "print('WARNING!!! amount_of_weeks HAS BEEN CHANGED TO', amount_of_weeks)\n",
    "\n",
    "for week in range(amount_of_weeks):\n",
    "    print('week', week, len(weeks_clusters[week]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "week 0 hashcodes 194\n",
      "week 1 hashcodes 179\n",
      "week 2 hashcodes 178\n",
      "week 3 hashcodes 177\n",
      "week 4 hashcodes 165\n",
      "week 5 hashcodes 191\n",
      "week 6 hashcodes 190\n",
      "week 7 hashcodes 181\n",
      "week 8 hashcodes 165\n",
      "week 9 hashcodes 205\n",
      "week 10 hashcodes 182\n",
      "week 11 hashcodes 147\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "week_hashcodes = []\n",
    "for week, cluster_arr in enumerate(weeks_clusters):\n",
    "    week_hashcodes.append([])\n",
    "    for idx, df in enumerate(cluster_arr):\n",
    "        cluster = []\n",
    "        for row in df.iterrows():\n",
    "            index, data = row\n",
    "            cluster.append(data['name'].tolist())\n",
    "\n",
    "        cluster = list(set(cluster))\n",
    "\n",
    "        hashcodedingus = 0\n",
    "        for lamp in cluster:\n",
    "            hashcodedingus += pow(2, lamp)\n",
    "\n",
    "        if(len(cluster) > 1):\n",
    "            week_hashcodes[week].append(hashcodedingus)\n",
    "\n",
    "for week in range(amount_of_weeks):\n",
    "    print('week', week, 'hashcodes', len(week_hashcodes[week]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{20: {'occurance_week': {'0': 33,\n",
       "   '1': 31,\n",
       "   '2': 27,\n",
       "   '3': 26,\n",
       "   '4': 19,\n",
       "   '5': 30,\n",
       "   '6': 30,\n",
       "   '7': 28,\n",
       "   '8': 24,\n",
       "   '9': 36,\n",
       "   '10': 22,\n",
       "   '11': 16}},\n",
       " 17: {'occurance_week': {'0': 9,\n",
       "   '1': 8,\n",
       "   '2': 8,\n",
       "   '3': 12,\n",
       "   '4': 8,\n",
       "   '5': 10,\n",
       "   '6': 7,\n",
       "   '7': 11,\n",
       "   '8': 9,\n",
       "   '9': 10,\n",
       "   '10': 2,\n",
       "   '11': 9}},\n",
       " 5: {'occurance_week': {'0': 30,\n",
       "   '1': 30,\n",
       "   '2': 24,\n",
       "   '3': 35,\n",
       "   '4': 23,\n",
       "   '5': 27,\n",
       "   '6': 22,\n",
       "   '7': 32,\n",
       "   '8': 27,\n",
       "   '9': 21,\n",
       "   '10': 30,\n",
       "   '11': 28}},\n",
       " 25: {'occurance_week': {'0': 1,\n",
       "   '1': 1,\n",
       "   '2': 2,\n",
       "   '3': 0,\n",
       "   '4': 0,\n",
       "   '5': 0,\n",
       "   '6': 1,\n",
       "   '7': 0,\n",
       "   '8': 2,\n",
       "   '9': 0,\n",
       "   '10': 1,\n",
       "   '11': 0}},\n",
       " 6: {'occurance_week': {'0': 32,\n",
       "   '1': 26,\n",
       "   '2': 18,\n",
       "   '3': 27,\n",
       "   '4': 19,\n",
       "   '5': 24,\n",
       "   '6': 40,\n",
       "   '7': 24,\n",
       "   '8': 28,\n",
       "   '9': 34,\n",
       "   '10': 31,\n",
       "   '11': 20}},\n",
       " 18: {'occurance_week': {'0': 5,\n",
       "   '1': 6,\n",
       "   '2': 6,\n",
       "   '3': 5,\n",
       "   '4': 8,\n",
       "   '5': 8,\n",
       "   '6': 7,\n",
       "   '7': 6,\n",
       "   '8': 3,\n",
       "   '9': 9,\n",
       "   '10': 8,\n",
       "   '11': 6}},\n",
       " 12: {'occurance_week': {'0': 26,\n",
       "   '1': 15,\n",
       "   '2': 31,\n",
       "   '3': 18,\n",
       "   '4': 35,\n",
       "   '5': 23,\n",
       "   '6': 31,\n",
       "   '7': 20,\n",
       "   '8': 31,\n",
       "   '9': 31,\n",
       "   '10': 22,\n",
       "   '11': 21}},\n",
       " 10: {'occurance_week': {'0': 10,\n",
       "   '1': 7,\n",
       "   '2': 10,\n",
       "   '3': 11,\n",
       "   '4': 3,\n",
       "   '5': 7,\n",
       "   '6': 5,\n",
       "   '7': 9,\n",
       "   '8': 10,\n",
       "   '9': 8,\n",
       "   '10': 8,\n",
       "   '11': 8}},\n",
       " 24: {'occurance_week': {'0': 6,\n",
       "   '1': 7,\n",
       "   '2': 9,\n",
       "   '3': 8,\n",
       "   '4': 2,\n",
       "   '5': 15,\n",
       "   '6': 9,\n",
       "   '7': 7,\n",
       "   '8': 7,\n",
       "   '9': 7,\n",
       "   '10': 8,\n",
       "   '11': 8}},\n",
       " 14: {'occurance_week': {'0': 0,\n",
       "   '1': 5,\n",
       "   '2': 3,\n",
       "   '3': 5,\n",
       "   '4': 4,\n",
       "   '5': 5,\n",
       "   '6': 2,\n",
       "   '7': 8,\n",
       "   '8': 1,\n",
       "   '9': 3,\n",
       "   '10': 5,\n",
       "   '11': 2}},\n",
       " 22: {'occurance_week': {'0': 4,\n",
       "   '1': 2,\n",
       "   '2': 5,\n",
       "   '3': 4,\n",
       "   '4': 8,\n",
       "   '5': 2,\n",
       "   '6': 3,\n",
       "   '7': 3,\n",
       "   '8': 3,\n",
       "   '9': 3,\n",
       "   '10': 2,\n",
       "   '11': 1}},\n",
       " 9: {'occurance_week': {'0': 8,\n",
       "   '1': 12,\n",
       "   '2': 13,\n",
       "   '3': 4,\n",
       "   '4': 9,\n",
       "   '5': 12,\n",
       "   '6': 10,\n",
       "   '7': 5,\n",
       "   '8': 6,\n",
       "   '9': 11,\n",
       "   '10': 10,\n",
       "   '11': 7}},\n",
       " 21: {'occurance_week': {'0': 3,\n",
       "   '1': 7,\n",
       "   '2': 1,\n",
       "   '3': 2,\n",
       "   '4': 3,\n",
       "   '5': 4,\n",
       "   '6': 0,\n",
       "   '7': 3,\n",
       "   '8': 3,\n",
       "   '9': 0,\n",
       "   '10': 3,\n",
       "   '11': 3}},\n",
       " 13: {'occurance_week': {'0': 2,\n",
       "   '1': 3,\n",
       "   '2': 6,\n",
       "   '3': 3,\n",
       "   '4': 3,\n",
       "   '5': 1,\n",
       "   '6': 3,\n",
       "   '7': 4,\n",
       "   '8': 1,\n",
       "   '9': 3,\n",
       "   '10': 5,\n",
       "   '11': 3}},\n",
       " 3: {'occurance_week': {'0': 4,\n",
       "   '1': 10,\n",
       "   '2': 6,\n",
       "   '3': 8,\n",
       "   '4': 5,\n",
       "   '5': 16,\n",
       "   '6': 7,\n",
       "   '7': 9,\n",
       "   '8': 5,\n",
       "   '9': 16,\n",
       "   '10': 6,\n",
       "   '11': 6}},\n",
       " 7: {'occurance_week': {'0': 1,\n",
       "   '1': 2,\n",
       "   '2': 2,\n",
       "   '3': 3,\n",
       "   '4': 5,\n",
       "   '5': 3,\n",
       "   '6': 5,\n",
       "   '7': 3,\n",
       "   '8': 2,\n",
       "   '9': 3,\n",
       "   '10': 3,\n",
       "   '11': 0}},\n",
       " 26: {'occurance_week': {'0': 1,\n",
       "   '1': 1,\n",
       "   '2': 0,\n",
       "   '3': 1,\n",
       "   '4': 2,\n",
       "   '5': 0,\n",
       "   '6': 1,\n",
       "   '7': 2,\n",
       "   '8': 0,\n",
       "   '9': 2,\n",
       "   '10': 3,\n",
       "   '11': 1}},\n",
       " 28: {'occurance_week': {'0': 1,\n",
       "   '1': 4,\n",
       "   '2': 4,\n",
       "   '3': 1,\n",
       "   '4': 3,\n",
       "   '5': 2,\n",
       "   '6': 2,\n",
       "   '7': 3,\n",
       "   '8': 0,\n",
       "   '9': 5,\n",
       "   '10': 2,\n",
       "   '11': 5}},\n",
       " 19: {'occurance_week': {'0': 0,\n",
       "   '1': 1,\n",
       "   '2': 1,\n",
       "   '3': 0,\n",
       "   '4': 1,\n",
       "   '5': 0,\n",
       "   '6': 0,\n",
       "   '7': 1,\n",
       "   '8': 0,\n",
       "   '9': 1,\n",
       "   '10': 4,\n",
       "   '11': 1}},\n",
       " 15: {'occurance_week': {'0': 0,\n",
       "   '1': 0,\n",
       "   '2': 0,\n",
       "   '3': 1,\n",
       "   '4': 2,\n",
       "   '5': 0,\n",
       "   '6': 0,\n",
       "   '7': 1,\n",
       "   '8': 1,\n",
       "   '9': 0,\n",
       "   '10': 0,\n",
       "   '11': 0}},\n",
       " 11: {'occurance_week': {'0': 0,\n",
       "   '1': 0,\n",
       "   '2': 0,\n",
       "   '3': 1,\n",
       "   '4': 2,\n",
       "   '5': 1,\n",
       "   '6': 4,\n",
       "   '7': 1,\n",
       "   '8': 2,\n",
       "   '9': 0,\n",
       "   '10': 1,\n",
       "   '11': 2}},\n",
       " 29: {'occurance_week': {'0': 0,\n",
       "   '1': 0,\n",
       "   '2': 0,\n",
       "   '3': 0,\n",
       "   '4': 1,\n",
       "   '5': 0,\n",
       "   '6': 0,\n",
       "   '7': 0,\n",
       "   '8': 0,\n",
       "   '9': 1,\n",
       "   '10': 2,\n",
       "   '11': 0}},\n",
       " 27: {'occurance_week': {'0': 0,\n",
       "   '1': 0,\n",
       "   '2': 0,\n",
       "   '3': 0,\n",
       "   '4': 0,\n",
       "   '5': 0,\n",
       "   '6': 0,\n",
       "   '7': 0,\n",
       "   '8': 0,\n",
       "   '9': 0,\n",
       "   '10': 1,\n",
       "   '11': 0}},\n",
       " 23: {'occurance_week': {'0': 0,\n",
       "   '1': 0,\n",
       "   '2': 0,\n",
       "   '3': 0,\n",
       "   '4': 0,\n",
       "   '5': 0,\n",
       "   '6': 0,\n",
       "   '7': 1,\n",
       "   '8': 0,\n",
       "   '9': 0,\n",
       "   '10': 2,\n",
       "   '11': 0}},\n",
       " 30: {'occurance_week': {'0': 0,\n",
       "   '1': 0,\n",
       "   '2': 0,\n",
       "   '3': 0,\n",
       "   '4': 0,\n",
       "   '5': 0,\n",
       "   '6': 0,\n",
       "   '7': 0,\n",
       "   '8': 0,\n",
       "   '9': 1,\n",
       "   '10': 1,\n",
       "   '11': 0}}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_dict = {}\n",
    "for week, hashcodes_arr in enumerate(week_hashcodes):\n",
    "    for i in hashcodes_arr:\n",
    "        if i in count_dict:\n",
    "            # count_dict[i]['occurance_week_' + str(week)] += 1\n",
    "            count_dict[i]['occurance_week'][str(week)] += 1\n",
    "        else:\n",
    "            count_dict[i] = {}\n",
    "            count_dict[i]['occurance_week'] = {}\n",
    "            for w in range(amount_of_weeks):\n",
    "                # count_dict[i]['occurance_week_' + str(w)] = 0\n",
    "                count_dict[i]['occurance_week'][str(w)] = 0\n",
    "\n",
    "count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3: {'is_predicted_group_percentage': 29.4,\n",
      "     'is_relevant_group_percentage': 25.69},\n",
      " 5: {'is_predicted_group_percentage': 95.0,\n",
      "     'is_relevant_group_percentage': 91.97},\n",
      " 6: {'is_predicted_group_percentage': 95.0,\n",
      "     'is_relevant_group_percentage': 87.7},\n",
      " 7: {'is_predicted_group_percentage': 9.6,\n",
      "     'is_relevant_group_percentage': 8.03},\n",
      " 9: {'is_predicted_group_percentage': 32.1,\n",
      "     'is_relevant_group_percentage': 34.05},\n",
      " 10: {'is_predicted_group_percentage': 28.8,\n",
      "      'is_relevant_group_percentage': 30.69},\n",
      " 11: {'is_predicted_group_percentage': 4.2,\n",
      "      'is_relevant_group_percentage': 1.95},\n",
      " 12: {'is_predicted_group_percentage': 95.0,\n",
      "      'is_relevant_group_percentage': 81.68},\n",
      " 13: {'is_predicted_group_percentage': 11.1,\n",
      "      'is_relevant_group_percentage': 10.98},\n",
      " 14: {'is_predicted_group_percentage': 12.9,\n",
      "      'is_relevant_group_percentage': 10.75},\n",
      " 15: {'is_predicted_group_percentage': 1.5,\n",
      "      'is_relevant_group_percentage': 1.08},\n",
      " 17: {'is_predicted_group_percentage': 30.9,\n",
      "      'is_relevant_group_percentage': 31.9},\n",
      " 18: {'is_predicted_group_percentage': 23.1,\n",
      "      'is_relevant_group_percentage': 21.21},\n",
      " 19: {'is_predicted_group_percentage': 3.0,\n",
      "      'is_relevant_group_percentage': 1.86},\n",
      " 20: {'is_predicted_group_percentage': 95.0,\n",
      "      'is_relevant_group_percentage': 92.47},\n",
      " 21: {'is_predicted_group_percentage': 9.6,\n",
      "      'is_relevant_group_percentage': 12.03},\n",
      " 22: {'is_predicted_group_percentage': 12.0,\n",
      "      'is_relevant_group_percentage': 13.63},\n",
      " 23: {'is_predicted_group_percentage': 0.9,\n",
      "      'is_relevant_group_percentage': 0.16},\n",
      " 24: {'is_predicted_group_percentage': 27.9,\n",
      "      'is_relevant_group_percentage': 26.08},\n",
      " 25: {'is_predicted_group_percentage': 2.4,\n",
      "      'is_relevant_group_percentage': 3.2},\n",
      " 26: {'is_predicted_group_percentage': 4.2,\n",
      "      'is_relevant_group_percentage': 3.28},\n",
      " 27: {'is_predicted_group_percentage': 0.3,\n",
      "      'is_relevant_group_percentage': 0.03},\n",
      " 28: {'is_predicted_group_percentage': 9.6,\n",
      "      'is_relevant_group_percentage': 8.78},\n",
      " 29: {'is_predicted_group_percentage': 1.2,\n",
      "      'is_relevant_group_percentage': 0.38},\n",
      " 30: {'is_predicted_group_percentage': 0.6,\n",
      "      'is_relevant_group_percentage': 0.08}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for key,val in count_dict.items():\n",
    "    threshold = week_threshold * amount_of_weeks\n",
    "    \n",
    "    total_occurances = 0\n",
    "    for week in range(amount_of_weeks):\n",
    "        total_occurances += val['occurance_week'][str(week)]\n",
    "        \n",
    "    if total_occurances >= threshold:\n",
    "        div = (total_occurances / threshold)\n",
    "        count = 1\n",
    "        perc = threshold_perc\n",
    "        \n",
    "        while div > 1:\n",
    "            div /= 2\n",
    "            perc += ((100 - threshold_perc) / 2) * (1 / count)\n",
    "            count *= 2\n",
    "            \n",
    "    else:\n",
    "        perc = (total_occurances / threshold) * threshold_perc\n",
    "    \n",
    "    count_dict[key]['is_predicted_group_percentage'] = round(perc, 2)\n",
    "\n",
    "\n",
    "for key,val in count_dict.items():\n",
    "    total = 0\n",
    "    current = 0\n",
    "    for week in range(amount_of_weeks):\n",
    "        \n",
    "        perc = 0\n",
    "        if val['occurance_week'][str(week)] >= week_threshold:\n",
    "            div = (val['occurance_week'][str(week)] / week_threshold)\n",
    "            count = 1\n",
    "            perc = threshold_perc\n",
    "            while div > 1:\n",
    "                div /= 2\n",
    "                perc += ((100 - threshold_perc) / 2) * (1 / count)\n",
    "                count *= 2\n",
    "        else:\n",
    "            perc = (val['occurance_week'][str(week)] / week_threshold) * threshold_perc\n",
    "        \n",
    "        total += 100 * (0.5) / pow(2, week * relevance_decay_strength)\n",
    "        current += perc * (0.5) / pow(2, week * relevance_decay_strength)\n",
    "\n",
    "    count_dict[key]['is_relevant_group_percentage'] = round((current / total) * 100, 2)\n",
    "    \n",
    "    count_dict[key].pop('occurance_week', None)\n",
    "\n",
    "pprint.pprint(count_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
