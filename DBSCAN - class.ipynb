{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pylab import rcParams\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.cluster import DBSCAN\n",
    "from collections import Counter\n",
    "import datetime\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from functools import reduce\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_milliseconds = lambda seconds : seconds * 60 * 1000\n",
    "\n",
    "class BinaryDataAnalysis:\n",
    "    \"\"\"Convert non-nummeric values in the dataframe to numbers so that the dataframe can be used to fit a model\n",
    "\n",
    "    Args (all optional):\n",
    "        eps: The epsilon in minutes (starting minimum distance between datapoints to cluster them together)\n",
    "        cluster_degregation: The next epsilon divider to use if clusters are too large \n",
    "                             (if eps=5 and cluster_degregation=2 then the next eps will be 2.5, and the next 1.25 etc.)\n",
    "        max_cluster_distance: the maximum size of a cluster in minutes\n",
    "        weeks: the amount of weeks to analyze, \n",
    "               a minimum of 1 needed, \n",
    "               a minimum of 2 is recommended\n",
    "        decay_strength: how much the next week counts for predicting relevant groups\n",
    "                        e.g. with a decay_strength of 0.5, ech week before last week will \n",
    "                             count half as strong for predicting if the groups are still relevant\n",
    "        cluster_threshold: from how many occourences (in one week) should it get 'self.threshold_percentage' \n",
    "                           as percentage that it is a group...\n",
    "                           More occourences will result in a higher persentage than 'self.threshold_percentage'\n",
    "                           Less occourences will result in a lower persentage\n",
    "        threshold_percentage: the persentage to give a group if the amount of occourences is 'self.cluster_threshold'\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 eps=5,\n",
    "                 cluster_degregation=2,\n",
    "                 max_cluster_distance=7.5, #minutes\n",
    "                 weeks=5,\n",
    "                 decay_strength=0.5,\n",
    "                 cluster_threshold=25,\n",
    "                 threshold_percentage=90):\n",
    "        self.eps =                  eps\n",
    "        self.cluster_degregation =  cluster_degregation\n",
    "        self.max_cluster_distance = max_cluster_distance\n",
    "        self.weeks =                weeks\n",
    "        self.decay_strength =       decay_strength\n",
    "        self.cluster_threshold =    cluster_threshold\n",
    "        self.threshold_percentage = threshold_percentage\n",
    "        \n",
    "    def analyze(self, df):\n",
    "        \"\"\"Analyze a dataframe and return a list of predicted groups & relevant groups\n",
    "\n",
    "        Args:\n",
    "            df: the dataframe to analyze\n",
    "\n",
    "        Returns:\n",
    "            result: an array of predicted groups in the following format:\n",
    "                [\n",
    "                    {\n",
    "                        item_ids: a list of item-id's that are predicted to be a group,\n",
    "                        is_predicted_group_percentage: the percentage chance that this is a group,\n",
    "                        is_relevant_group_percentage: the percentage chance that this group is still relevant \n",
    "                                                      (depending on how much it has been used lately)\n",
    "                    },\n",
    "                    {...},\n",
    "                    {...}\n",
    "                ]\n",
    "        \"\"\"\n",
    "        # todo? cut off trailing days?\n",
    "        \n",
    "        self.lookup_table = self.create_lookup_table(\n",
    "            df=df\n",
    "        )\n",
    "        df_fit = self.clean_dataframe(\n",
    "            df=df\n",
    "        )\n",
    "        week_hashcodes = self.get_week_clusters_hash_codes(\n",
    "            df=df_fit\n",
    "        )\n",
    "        hashcode_occurances = self.get_hashcode_occurances_per_week(\n",
    "            week_hashcodes=week_hashcodes\n",
    "        )\n",
    "        predicted_groups = self.calculate_groups(\n",
    "            hashcode_occurances_per_week=hashcode_occurances\n",
    "        )\n",
    "        \n",
    "        result = []\n",
    "        for key in predicted_groups:\n",
    "            items = self.get_lookup_values(\n",
    "                hashcode=key\n",
    "            )\n",
    "            result.append({\n",
    "                'item_ids': items,\n",
    "                'is_predicted_group_percentage': predicted_groups[key]['is_predicted_group_percentage'],\n",
    "                'is_relevant_group_percentage': predicted_groups[key]['is_relevant_group_percentage']\n",
    "            })\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def create_lookup_table(self, df):\n",
    "        \"\"\"Creates a lookup table for all unique row-id's\n",
    "\n",
    "        Args:\n",
    "            df: the dataframe containing an id column with several diffrent devices creating events\n",
    "\n",
    "        Returns:\n",
    "            lookup_dict: a dictionary where each id corresponds to an index e.g.\n",
    "                         { 0: 1743, 1: 1749, 2: 1803, 3: 1890, 4: 1911}\n",
    "        \"\"\"\n",
    "        df_lookup = pd.DataFrame(data={ 'id': pd.Series(df['id']).unique() })\n",
    "        \n",
    "        df_lookup['hashcode'] = self.clean_dataframe(\n",
    "            df=df_lookup\n",
    "        )['id']\n",
    "        lookup_dict = dict()\n",
    "        for index, row in df_lookup.iterrows():\n",
    "            lookup_dict[row['hashcode']] = row['id']\n",
    "        return lookup_dict\n",
    "    \n",
    "    def clean_dataframe(self, df):\n",
    "        \"\"\"Convert non-nummeric values in the dataframe to numbers so that the dataframe can be used to fit a model\n",
    "\n",
    "        Args:\n",
    "            df: The dataframe to clean.\n",
    "\n",
    "        Returns:\n",
    "            df_fit: The dataframe with nummeric values\n",
    "        \"\"\"\n",
    "        d = defaultdict(LabelEncoder)\n",
    "        df_fit = df.apply(lambda x: d[x.name].fit_transform(x))\n",
    "        if 'state' in df.columns:\n",
    "            df_fit['state'] = df['state']\n",
    "        if 'time' in df.columns:\n",
    "            df_fit['time'] = df['time']\n",
    "        return df_fit\n",
    "    \n",
    "    def get_week_clusters_hash_codes(self, df):\n",
    "        \"\"\"Get Cluster for a dataframe per week\n",
    "\n",
    "        Args:\n",
    "            df: The dataframe with more than one week of timestamps to cluster.\n",
    "\n",
    "        Returns:\n",
    "            week_hashcodes: A multidimentional array where each array is one week, and in one week array \n",
    "                            are a list of clusters represented by a hashcode.\n",
    "                            \n",
    "                            A hashcode is the reversed binary representation of a cluster, \n",
    "                            e.g. \n",
    "                            hashcode 3\n",
    "                            is binary 00000011\n",
    "                            is reversed 11000000\n",
    "                            means devices with index 0 and 1 (from the lookup table) are grouped\n",
    "                            \n",
    "                            Example output:\n",
    "                                [[3, 5, 20], [3, 3, 20]]\n",
    "                            means:\n",
    "                                amount of weeks: 2\n",
    "                                clusters in week 1:\n",
    "                                    3  (00000011) = a group with device 0 & 1\n",
    "                                    5  (00000101) = a group with device 0 & 2\n",
    "                                    20 (00010100) = a group with device 2 & 4\n",
    "                                clusters in week 2:\n",
    "                                    3  (00000011) = a group with device 0 & 1\n",
    "                                    3  (00000011) = another group with device 0 & 1\n",
    "                                    21 (00010101) = a group with device 0, 2 & 4\n",
    "        \"\"\"\n",
    "        one_week_in_milliseconds = (1000 * 60 * 60 * 24 * 7)\n",
    "        last_timestamp = df['time'].max()\n",
    "        week_hashcodes = []\n",
    "        for week in range(self.weeks):\n",
    "            week_hashcodes.append([])\n",
    "            df_week = df[df['time'] >= last_timestamp - ((week + 1) * one_week_in_milliseconds)]\n",
    "            df_week = df_week[df_week['time'] < last_timestamp - (week * one_week_in_milliseconds)]\n",
    "\n",
    "            if not df_week.empty:\n",
    "                cluster_arr = self.split_dataframe_on_state_and_get_cluster_arr(\n",
    "                    df=df_week, \n",
    "                    starting_eps=self.eps\n",
    "                )\n",
    "                for idx, df_week in enumerate(cluster_arr):\n",
    "                    cluster = []\n",
    "                    for row in df_week.iterrows():\n",
    "                        index, data = row\n",
    "                        cluster.append(data['id'].tolist())\n",
    "\n",
    "                    cluster = list(set(cluster))\n",
    "\n",
    "                    hashcode = 0\n",
    "                    for lamp in cluster:\n",
    "                        hashcode += pow(2, lamp)\n",
    "\n",
    "                    if(len(cluster) > 1):\n",
    "                        week_hashcodes[week].append(hashcode)\n",
    "            else:\n",
    "                print(\n",
    "                    'WARNING!!! There are not', \n",
    "                    self.weeks, \n",
    "                    'weeks in the dataset... amount_of_weeks HAS BEEN CHANGED TO', \n",
    "                    week\n",
    "                )\n",
    "                self.weeks = week\n",
    "                break\n",
    "        return week_hashcodes\n",
    "    \n",
    "    def split_dataframe_on_state_and_get_cluster_arr(self, df, starting_eps):\n",
    "        \"\"\"Split a dataframe into 2 seperate dataframes (one with state=0, the other with state=1) \n",
    "           and get the clusters for both of the dataframes\n",
    "\n",
    "        Args:\n",
    "            df: The dataframe to split & get clusters from.\n",
    "\n",
    "        Returns:\n",
    "            cluster_arr: an array that holds 0 or more dataframes (clusters)\n",
    "        \"\"\"\n",
    "        df_1 = df.loc[df['state'] == 1]\n",
    "        df_0 = df.loc[df['state'] == 0]\n",
    "        cluster_arr1 = self.get_clusters_recursive(df=df_1.copy(), eps=self.eps)\n",
    "        cluster_arr2 = self.get_clusters_recursive(df=df_0.copy(), eps=self.eps)\n",
    "        cluster_arr = cluster_arr1 + cluster_arr2\n",
    "        return cluster_arr\n",
    "    \n",
    "    def get_clusters_recursive(self, df, eps, iteration=0, cluster_arr=None):\n",
    "        \"\"\"Get clusters for a single dataframe\n",
    "           \n",
    "        Args:\n",
    "            df: The dataframe\n",
    "            eps: the epsilon to start with (maximum distance between two datapoints)\n",
    "\n",
    "        Returns:\n",
    "            cluster_arr: An array of dataframes (each one represents a cluster)e.g.\n",
    "                         [DataFrame, DataFrame, DataFrame, ...]\n",
    "        \"\"\"\n",
    "        if cluster_arr is None:\n",
    "            cluster_arr = []\n",
    "        \n",
    "        model = self.fit_model(df, eps)\n",
    "        cluster_dict = self.get_clusters(df=df, model=model)\n",
    "        \n",
    "        for idx, df in cluster_dict['too_large'].items():\n",
    "            cluster_arr + self.get_clusters_recursive(\n",
    "                df=cluster_dict['too_large'][idx], \n",
    "                eps=eps / self.cluster_degregation, \n",
    "                iteration=iteration + 1, \n",
    "                cluster_arr=cluster_arr\n",
    "            )\n",
    "    \n",
    "        for idx, df in cluster_dict['perfect_size'].items():\n",
    "            cluster_arr.append(df)\n",
    "        return cluster_arr\n",
    "    \n",
    "    \n",
    "    def fit_model(self, df, eps):\n",
    "        \"\"\"Fit the dataframe in the DBSCAN algoritm and return the model\n",
    "           \n",
    "           more information: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html\n",
    "           \n",
    "        Args:\n",
    "            df: The dataframe to run the algorithm on\n",
    "            eps: the epsilon (maximum distance between two datapoints)\n",
    "\n",
    "        Returns:\n",
    "            model: The fitted DBSCAN model\n",
    "        \"\"\"\n",
    "        model = DBSCAN(\n",
    "            eps=to_milliseconds(eps),\n",
    "            min_samples=2\n",
    "        ).fit(df)\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def get_clusters(self, df, model):\n",
    "        \"\"\"Get clusters for a single dataframe\n",
    "           \n",
    "        Args:\n",
    "            df: The dataframe\n",
    "            model: the fitted model\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary with 2 keys, each of wich is another dictionary which contains all dataframes (one per cluster)\n",
    "            e.g.\n",
    "            {\n",
    "                'too_large': {\n",
    "                    0: DataFrame,\n",
    "                    1: DataFrame,\n",
    "                    2: DataFrame\n",
    "                },\n",
    "                'perfect_size': {\n",
    "                    0: DataFrame,\n",
    "                    1: DataFrame\n",
    "                }\n",
    "            }\n",
    "        \"\"\"\n",
    "        df['cluster'] = model.labels_\n",
    "        \n",
    "        cluster_dict_too_large = {}\n",
    "        cluster_dict_perfect_size = {}\n",
    "        \n",
    "        \n",
    "        # Calculate amount of clusters\n",
    "        cluster_data_count = Counter(model.labels_)\n",
    "        if -1 in cluster_data_count:\n",
    "            cluster_data_count.pop(-1) # don't count outliers as a cluster\n",
    "        if (bool(cluster_data_count)):\n",
    "            amount_of_clusters = max(cluster_data_count) + 1\n",
    "        else:\n",
    "            amount_of_clusters = 0;\n",
    "        \n",
    "        \n",
    "        for idx in range(amount_of_clusters):\n",
    "            cluster_df = df.loc[df['cluster'] == idx].drop(columns=['cluster'])\n",
    "            \n",
    "            first_time = cluster_df['time'].iloc[0]\n",
    "            last_time = cluster_df['time'].iloc[cluster_df['time'].size - 1]\n",
    "            diffrence_in_miliseconds = last_time - first_time\n",
    "            if diffrence_in_miliseconds > to_milliseconds(self.max_cluster_distance):\n",
    "                cluster_dict_too_large[idx] = cluster_df\n",
    "            else:\n",
    "                cluster_dict_perfect_size[idx] = cluster_df\n",
    "        \n",
    "        return {\n",
    "            'too_large': cluster_dict_too_large,\n",
    "            'perfect_size': cluster_dict_perfect_size\n",
    "        }\n",
    "        \n",
    "    \n",
    "    def get_hashcode_occurances_per_week(self, week_hashcodes):\n",
    "        \"\"\"Count all occourences of hashcodes per week\n",
    "            \n",
    "         Args:\n",
    "             week_hashcodes: The week_hashcodes (generated from self.get_week_clusters_hash_codes())\n",
    "\n",
    "         Returns:\n",
    "             count_dict: A dictionary with an index for each hashcode, with all\n",
    "                         occourences per week (last week = 0, the week before that = 1).\n",
    "             e.g.\n",
    "             {\n",
    "                 '3': {\n",
    "                     'occurance_week': {\n",
    "                         '0': 24,\n",
    "                         '1': 56,\n",
    "                         '2': 32,\n",
    "                         '3': 12\n",
    "                     }\n",
    "                 },\n",
    "                 '5': { 'occurance_week': { ... } },\n",
    "                 '20': { 'occurance_week': { ... } },\n",
    "                 ...\n",
    "             }\n",
    "        \"\"\"\n",
    "        count_dict = {}\n",
    "        for week, hashcodes_arr in enumerate(week_hashcodes):\n",
    "            for i in hashcodes_arr:\n",
    "                if i in count_dict:\n",
    "                    count_dict[i]['occurance_week'][str(week)] += 1\n",
    "                else:\n",
    "                    count_dict[i] = {}\n",
    "                    count_dict[i]['occurance_week'] = {}\n",
    "                    for w in range(self.weeks):\n",
    "                        count_dict[i]['occurance_week'][str(w)] = 0\n",
    "        return count_dict\n",
    "    \n",
    "    def calculate_groups(self, hashcode_occurances_per_week):\n",
    "        \"\"\"Calculate the predicted groups & relevant groups persentages from the amount of occourences.\n",
    "            \n",
    "         Args:\n",
    "             hashcode_occurances_per_week: The hashcode occurances per week \n",
    "                                           (generated from self.get_hashcode_occurances_per_week())\n",
    "\n",
    "         Returns:\n",
    "             count_dict: A dictionary with an index for each hashcode and the predicted groups & relevant groups persentages\n",
    "             e.g.\n",
    "             {\n",
    "                 '3': {\n",
    "                     'is_predicted_group_percentage': 92.3,\n",
    "                     'is_relevant_group_percentage': 72.1,\n",
    "                 },\n",
    "                 '5': { \n",
    "                     'is_predicted_group_percentage': 42.9,\n",
    "                     'is_relevant_group_percentage': 51.8,\n",
    "                 },\n",
    "                 '20': { ... },\n",
    "                 ...\n",
    "             }\n",
    "        \"\"\"\n",
    "        count_dict = hashcode_occurances_per_week\n",
    "        for key,val in count_dict.items():\n",
    "            threshold = self.cluster_threshold * self.weeks\n",
    "\n",
    "            total_occurances = 0\n",
    "            for week in range(self.weeks):\n",
    "                total_occurances += val['occurance_week'][str(week)]\n",
    "\n",
    "            if total_occurances >= threshold:\n",
    "                div = (total_occurances / threshold)\n",
    "                count = 1\n",
    "                perc = self.threshold_percentage\n",
    "\n",
    "                while div > 1:\n",
    "                    div /= 2\n",
    "                    perc += ((100 - self.threshold_percentage) / 2) * (1 / count)\n",
    "                    count *= 2\n",
    "\n",
    "            else:\n",
    "                perc = (total_occurances / threshold) * self.threshold_percentage\n",
    "\n",
    "            count_dict[key]['is_predicted_group_percentage'] = round(perc, 2)\n",
    "\n",
    "\n",
    "        for key,val in count_dict.items():\n",
    "            total = 0\n",
    "            current = 0\n",
    "            for week in range(self.weeks):\n",
    "\n",
    "                perc = 0\n",
    "                if val['occurance_week'][str(week)] >= self.cluster_threshold:\n",
    "                    div = (val['occurance_week'][str(week)] / self.cluster_threshold)\n",
    "                    count = 1\n",
    "                    perc = self.threshold_percentage\n",
    "                    while div > 1:\n",
    "                        div /= 2\n",
    "                        perc += ((100 - self.threshold_percentage) / 2) * (1 / count)\n",
    "                        count *= 2\n",
    "                else:\n",
    "                    perc = (val['occurance_week'][str(week)] / self.cluster_threshold) * self.threshold_percentage\n",
    "\n",
    "                total += 100 * (0.5) / pow(2, week * self.decay_strength)\n",
    "                current += perc * (0.5) / pow(2, week * self.decay_strength)\n",
    "\n",
    "            count_dict[key]['is_relevant_group_percentage'] = round((current / total) * 100, 2)\n",
    "            count_dict[key].pop('occurance_week', None)\n",
    "        return count_dict\n",
    "    \n",
    "    def get_lookup_values(self, hashcode):\n",
    "        \"\"\"Get the individual item-indexes for a given hashcode\n",
    "\n",
    "        Args:\n",
    "            hashcode: The dataframe hashcode\n",
    "                      e.g. 21\n",
    "\n",
    "        Returns:\n",
    "            items: An array of items\n",
    "                   e.g. [0, 2, 4]\n",
    "        \n",
    "        e.g.\n",
    "            hashcode 21 \n",
    "            = 00010101 in binary \n",
    "            = 10101000 reversed \n",
    "            =   item 0 = true,\n",
    "                item 1 = false\n",
    "                item 2 = true\n",
    "                item 3 = false\n",
    "                item 4 = true\n",
    "                item 5 = false\n",
    "                item 6 = false\n",
    "                item 7 = false\n",
    "            = a group with device 0, 2 & 4\n",
    "        \"\"\"\n",
    "        def bitfield(n):\n",
    "            return [int(digit) for digit in bin(n)[2:]]\n",
    "        \n",
    "        bits = bitfield(hashcode)[::-1]\n",
    "        \n",
    "        items = []\n",
    "        for idx, bit in enumerate(bits):\n",
    "            if bit == 1:\n",
    "                items.append(self.lookup_table[idx])\n",
    "        return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameValidator:\n",
    "    \"\"\"Validate a Dataframe for use in BinaryDataAnalisys\n",
    "       Be aware: the values for the time column are expected to be timestamps in milliseconds\n",
    "    \"\"\"\n",
    "    time_column = 'time'\n",
    "    expected_columns = ['id', 'state', time_column]\n",
    "    minimum_days_of_data_needed = 7\n",
    "    \n",
    "    def validate(self, df):\n",
    "        \"\"\"Validate a dataframe with the values specified above\n",
    "\n",
    "        Args:\n",
    "            df: the dataframe to validate\n",
    "\n",
    "        Returns:\n",
    "            boolean: if it's valid or not\n",
    "        \"\"\"\n",
    "        columns_valid = self.validate_columns(df)\n",
    "        if not columns_valid:\n",
    "            return False\n",
    "        \n",
    "        min_amount_of_data_valid = self.validate_minimum_days_of_data_needed(df)\n",
    "        if not min_amount_of_data_valid:\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def validate_columns(self, df):\n",
    "        \"\"\"Validate a dataframe's columns with the values specified above\n",
    "\n",
    "        Args:\n",
    "            df: the dataframe to validate\n",
    "\n",
    "        Returns:\n",
    "            boolean: if the columns are valid or not\n",
    "        \"\"\"\n",
    "        expected_df_columns = pd.DataFrame(columns=self.expected_columns)\n",
    "        \n",
    "        columns_too_many = df.columns.difference(expected_df_columns.columns)\n",
    "        if not len(columns_too_many) == 0:\n",
    "            print('The provided dataframe has too many columns:', *columns_too_many, sep='\\n')\n",
    "            \n",
    "        columns_too_few = expected_df_columns.columns.difference(df.columns)\n",
    "        if not len(columns_too_few) == 0:\n",
    "            print('The provided dataframe is missing the following columns:', *columns_too_few, sep='\\n')\n",
    "\n",
    "        return len(columns_too_many) + len(columns_too_few) == 0\n",
    "    \n",
    "    def validate_minimum_days_of_data_needed(self, df):\n",
    "        \"\"\"Validate a dataframe's amount of data with the values specified above\n",
    "\n",
    "        Args:\n",
    "            df: the dataframe to validate\n",
    "\n",
    "        Returns:\n",
    "            boolean: if the data is valid or not\n",
    "        \"\"\"\n",
    "        df_time = df.sort_values(by=[self.time_column])[self.time_column]\n",
    "        first_timestamp = df_time.values[0]\n",
    "        last_timestamp = df_time.values[-1]\n",
    "        diff = last_timestamp - first_timestamp\n",
    "        days = diff / 1000 / 60 / 60 / 24\n",
    "        enough_data = days > self.minimum_days_of_data_needed\n",
    "        \n",
    "        if not enough_data:\n",
    "            print(\n",
    "                'There is a minimum of ' + \n",
    "                str(self.minimum_days_of_data_needed) + \n",
    "                ' days of data needed, only ' + \n",
    "                str(math.floor(days * 100) / 100) + \n",
    "                ' days of data was given!'\n",
    "            )\n",
    "            \n",
    "        return enough_data\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get data & transform 'name' column into 'id' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(175000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>time</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1509490125797</td>\n",
       "      <td>Staande_Lamp_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1509490363420</td>\n",
       "      <td>Staande_Lamp_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1509491964532</td>\n",
       "      <td>Staande_Lamp_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1509492113970</td>\n",
       "      <td>Staande_Lamp_5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1509492126316</td>\n",
       "      <td>Staande_Lamp_3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   state           time              id\n",
       "1      1  1509490125797  Staande_Lamp_2\n",
       "2      1  1509490363420  Staande_Lamp_3\n",
       "0      0  1509491964532  Staande_Lamp_1\n",
       "6      1  1509492113970  Staande_Lamp_5\n",
       "4      0  1509492126316  Staande_Lamp_3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "address = './datasets/staandelamp_realistic_huge.json'\n",
    "df_data = pd.read_json(address)\n",
    "df_data = df_data.sort_values(by=['time'])\n",
    "df_data['id'] = df_data['name']\n",
    "df_data = df_data.drop(columns=['name'])\n",
    "print(df_data.shape)\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid!\n"
     ]
    }
   ],
   "source": [
    "validator = DataFrameValidator()\n",
    "dataframe_is_valid = validator.validate(df_data)\n",
    "\n",
    "if dataframe_is_valid:\n",
    "    print('Valid!')\n",
    "else:\n",
    "    print('WARNING! Dataframe validation failed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'item_ids': ['Staande_Lamp_1', 'Staande_Lamp_3'], 'is_predicted_group_percentage': 95.0, 'is_relevant_group_percentage': 93.27}, {'item_ids': ['Staande_Lamp_3', 'Staande_Lamp_4'], 'is_predicted_group_percentage': 95.0, 'is_relevant_group_percentage': 91.43}, {'item_ids': ['Staande_Lamp_2', 'Staande_Lamp_3'], 'is_predicted_group_percentage': 95.0, 'is_relevant_group_percentage': 85.54}, {'item_ids': ['Staande_Lamp_1', 'Staande_Lamp_3', 'Staande_Lamp_4'], 'is_predicted_group_percentage': 7.92, 'is_relevant_group_percentage': 7.76}, {'item_ids': ['Staande_Lamp_4', 'Staande_Lamp_5'], 'is_predicted_group_percentage': 28.08, 'is_relevant_group_percentage': 26.0}]\n"
     ]
    }
   ],
   "source": [
    "if dataframe_is_valid:\n",
    "    BDASCAN = BinaryDataAnalysis()\n",
    "    result = BDASCAN.analyze(df_data)\n",
    "    print(result[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
